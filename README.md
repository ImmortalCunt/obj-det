
# Real-Time Object Detection with YOLOv10

This repository showcases my project where I implemented a real-time object detection system using the YOLOv10 model. The goal was to create a system capable of identifying and labeling objects instantly from live webcam footage.

## About the Project
- **Framework**: YOLOv10 (You Only Look Once), a state-of-the-art object detection model
- **Tech Stack**: Python, OpenCV, Ultralytics YOLO library
- **Functionality**:
  - Captures live video from a webcam
  - Detects and classifies objects in real time
  - Displays bounding boxes, object names, and confidence levels on the video feed

This project was a hands-on exploration of AI and machine learning’s capabilities, particularly in computer vision applications.

## Why This Project?
As an agriculture undergraduate, I’m deeply interested in how AI can address real-world challenges in farming. This project helped me understand the technical side of object detection while inspiring me to explore its applications in agriculture.

## Future Work
Currently, I’m working on another exciting project: a model to detect the **ripening stages of tomato fruits**, which could have practical use in agriculture for optimizing harvest timing. Stay tuned for updates!

## Getting Started
1. Clone the repository:
   ```bash
   git clone https://github.com/ImmortalCunt/real-time-object-detection.git
   cd real-time-object-detection
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the `main.py` file:
   ```bash
   python main.py
   ```

## Files in this Repository
- `main.py`: Main Python script for real-time object detection
- `yolov10n.pt`: Pre-trained YOLOv10 model weights

## Requirements
- Python 3.8 or higher
- OpenCV
- cvzone
- Ultralytics YOLO library

## Contributions
Suggestions, feedback, or contributions are always welcome! Feel free to fork the repository, create a pull request, or open an issue.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
